{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report Generation Workflow\n",
    "\n",
    "Adapted directly from [this tutorial](https://github.com/run-llama/llamacloud-demo/blob/main/examples/report_generation/rfp_response/generate_rfp.ipynb) from LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import (\n",
    "    MetadataFilter,\n",
    "    MetadataFilters,\n",
    "    FilterOperator,\n",
    ")\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from pathlib import Path\n",
    "from typing import Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out_dir = \"data_out_dopeness\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_RETRIEVE_PREFIX = \"\"\"\\\n",
    "Synthesizes an answer to your question by feeding in in the entire relevant document as context. Best used for higher-level summarization options.\n",
    "Do NOT use if answer can be found in a specific chunk of a given document. Use the chunk_query_engine instead for that purpose.\n",
    "\n",
    "Document: {file_name}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_RETRIEVE_PREFIX = \"\"\"\\\n",
    "Synthesizes an answer to your question by feeding in relevant chunks of a document as context. Best used for questions that are more pointed in nature.\n",
    "Do NOT use if the question asks seems to require a general summary of any given document. Use the doc_query_engine instead for that purpose.\n",
    "\n",
    "Document: {file_name}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, Document, VectorStoreIndex, StorageContext, Settings\n",
    "from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from qdrant_client import QdrantClient\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from aiohttp import ClientTimeout\n",
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "def fetch_sitemap_urls(sitemap_url: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch all URLs from a sitemap XML file.\n",
    "    \n",
    "    Args:\n",
    "        sitemap_url (str): URL of the sitemap\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of URLs found in the sitemap\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(sitemap_url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the XML content\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        # Extract URLs (handles both regular sitemaps and sitemap indexes)\n",
    "        urls = []\n",
    "        \n",
    "        # Look for both standard sitemap URLs and sitemap index URLs\n",
    "        namespaces = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "        \n",
    "        # Get URLs from standard sitemap\n",
    "        for url in root.findall('.//ns:loc', namespaces):\n",
    "            urls.append(url.text)\n",
    "            \n",
    "        return urls\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching sitemap: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_page_async(url: str, session: aiohttp.ClientSession) -> Dict[str, str]:\n",
    "    \"\"\"Async version of page scraping.\"\"\"\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            content = await response.text()\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            for script in soup(['script', 'style']):\n",
    "                script.decompose()\n",
    "            text = soup.get_text()\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            return {\"url\": url, \"content\": ' '.join(chunk for chunk in chunks if chunk)}\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return {\"url\": url, \"content\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_urls(urls: List[str], domain: str, rate_limit: float) -> List[Document]:\n",
    "    \"\"\"Process URLs concurrently with rate limiting and progress bar.\"\"\"\n",
    "    timeout = ClientTimeout(total=30)\n",
    "    documents = []\n",
    "    semaphore = asyncio.Semaphore(50)\n",
    "    \n",
    "    async with aiohttp.ClientSession(timeout=timeout) as session:\n",
    "        async def process_with_rate_limit(url):\n",
    "            async with semaphore:\n",
    "                await asyncio.sleep(rate_limit)\n",
    "                result = await scrape_page_async(url, session)\n",
    "                if result[\"content\"]:\n",
    "                    return Document(\n",
    "                        text=result[\"content\"],\n",
    "                        metadata={\"source\": url, \"domain\": domain}\n",
    "                    )\n",
    "                return None\n",
    "        \n",
    "        tasks = [process_with_rate_limit(url) for url in urls]\n",
    "        results = await tqdm_asyncio.gather(*tasks, desc=\"Scraping pages\")\n",
    "        documents = [doc for doc in results if doc is not None]\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_from_sitemap(sitemap_url: str, collection_name: str = \"current_docs\", rate_limit: float = 0.10) -> VectorStoreIndex:\n",
    "    \"\"\"Create a LlamaIndex index from a sitemap using QDrant backend.\"\"\"\n",
    "    # Initialize QDrant client\n",
    "    client = QdrantClient(host=\"localhost\", port=6333)\n",
    "    vector_store = QdrantVectorStore(client=client, collection_name=collection_name)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    urls = fetch_sitemap_urls(sitemap_url)\n",
    "    if not urls:\n",
    "        raise ValueError(\"No URLs found in sitemap\")\n",
    "    \n",
    "    documents = []\n",
    "    domain = urlparse(sitemap_url).netloc\n",
    "    \n",
    "    documents = asyncio.run(process_urls(urls, domain, rate_limit))\n",
    "    \n",
    "    parser = SimpleNodeParser.from_defaults()\n",
    "    nodes = parser.get_nodes_from_documents(documents)\n",
    "    \n",
    "    # Create index with QDrant vector store\n",
    "    return VectorStoreIndex(nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"Snowflake/snowflake-arctic-embed-s\")\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "sitemap_url = \"https://docs.llamaindex.ai/en/stable/sitemap.xml\"\n",
    "index = create_index_from_sitemap(sitemap_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tool(\n",
    "    file: str, \n",
    "    file_description: Optional[str] = None,\n",
    "    retrieve_document: bool = False\n",
    "):\n",
    "    \"\"\"Return a function that retrieves information from the sitemap index.\"\"\"\n",
    "\n",
    "    def chunk_retriever_fn(query: str) -> str:\n",
    "        retriever = index.as_retriever(similarity_top_k=5)\n",
    "        nodes = retriever.retrieve(query)\n",
    "\n",
    "        full_text = \"\\n\\n========================\\n\\n\".join(\n",
    "            [n.get_content(metadata_mode=\"all\") for n in nodes]\n",
    "        )\n",
    "\n",
    "        return full_text\n",
    "\n",
    "    # define name as a function of the file\n",
    "    fn_name = Path(file).stem + \"_retrieve\"\n",
    "\n",
    "    tool_description_tmpl = DOC_RETRIEVE_PREFIX if retrieve_document else CHUNK_RETRIEVE_PREFIX\n",
    "    tool_description = tool_description_tmpl.format(file_name=file)\n",
    "    if file_description is not None:\n",
    "        tool_description += f\"\\n\\nFile Description: {file_description}\"\n",
    "\n",
    "    tool = FunctionTool.from_defaults(\n",
    "        fn=chunk_retriever_fn, name=fn_name, description=tool_description\n",
    "    )\n",
    "\n",
    "    return tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = []\n",
    "file_name = \"currentdocs\"\n",
    "summary = \"A collection of all the current documentation available on the LlamaIndex website.\"\n",
    "tools.append(generate_tool(file_name, file_description=summary))\n",
    "# document-level tool\n",
    "tools.append(\n",
    "    generate_tool(\n",
    "        file_name, \n",
    "        file_description=summary,\n",
    "        retrieve_document=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Synthesizes an answer to your question by feeding in relevant chunks of a document as context. Best used for questions that are more pointed in nature.\\nDo NOT use if the question asks seems to require a general summary of any given document. Use the doc_query_engine instead for that purpose.\\n\\nDocument: currentdocs\\n\\n\\nFile Description: A collection of all the current documentation available on the LlamaIndex website.', name='currentdocs_retrieve', fn_schema=<class 'llama_index.core.tools.utils.currentdocs_retrieve'>, return_direct=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Context,\n",
    "    Workflow,\n",
    "    step,\n",
    ")\n",
    "from llama_index.core.llms import LLM\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "import logging\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logger = logging.getLogger(__name__)\n",
    "_logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a research agent tasked with filling out a specific form key/question with the appropriate value, given a bank of context.\n",
    "You are given a specific form key/question. Think step-by-step and use the existing set of tools to help answer the question.\n",
    "\n",
    "You MUST always use at least one tool to answer each question. Only after you've determined that existing tools do not \\\n",
    "answer the question should you try to reason from first principles and prior knowledge to answer the question.\n",
    "\n",
    "You MUST try to answer the question instead of only saying 'I dont know'.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACT_KEYS_PROMPT = \"\"\"\\\n",
    "You are provided Dopeness Report.\n",
    "\n",
    "We wish to generate a response to the Dopeness Report in a way that adheres to the instructions within the Dopeness Report, \\\n",
    "including the specific sections that an Dopeness Report response should contain, and the content that would need to go \\\n",
    "into each section.\n",
    "\n",
    "Your task is to extract out a list of \"questions\", where each question corresponds to a specific section that is required in the Dopeness Report response.\n",
    "Put another way, after we extract out the questions we will go through each question and answer each one \\\n",
    "with our downstream research assistant, and the combined\n",
    "question:answer pairs will constitute the full Dopeness Report response.\n",
    "\n",
    "You must TRY to extract out questions that can be answered by the provided knowledge base. We provide the list of file metadata below. \n",
    "\n",
    "Additional requirements:\n",
    "- Try to make the questions SPECIFIC given your knowledge of the Dopeness Report and the knowledge base. Instead of asking a question like \\\n",
    "\"How do we ensure security\" ask a question that actually addresses a security requirement in the Dopeness Report and can be addressed by the knowledge base.\n",
    "- Make sure the questions are comprehensive and addresses all the Dopeness Report requirements.\n",
    "- Make sure each question is descriptive - this gives our downstream assistant context to fill out the value for that question \n",
    "- Extract out all the questions as a list of strings.\n",
    "\n",
    "Knowledge Base Files:\n",
    "{file_metadata}\n",
    "\n",
    "Dopeness Report Full Template:\n",
    "{dopeness_text}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_OUTPUT_PROMPT = \"\"\"\\\n",
    "You are an expert analyst.\n",
    "Your task is to generate a Dopeness Report response according to the given Dopeness Report and question/answer pairs.\n",
    "\n",
    "You are given the following Dopeness Report and qa pairs:\n",
    "\n",
    "<dopeness_document>\n",
    "{output_template}\n",
    "</dopeness_document>\n",
    "\n",
    "<question_answer_pairs>\n",
    "{answers}\n",
    "</question_answer_pairs>\n",
    "\n",
    "You cannot provide a 'Not specified' response. You must judge based on the *vibes* of the documentation - just make it up if you have to!\n",
    "\n",
    "Not every question has an appropriate answer. This is because the agent tasked with answering the question did not have the right context to answer it.\n",
    "If this is the case, you MUST come up with an answer that is reasonable. You CANNOT say that you are unsure in any area of the Dopeness Report response. \n",
    "\n",
    "Please generate the output according to the template and the answers, in markdown format.\n",
    "Directly output the generated markdown content, do not add any additional text, such as \"```markdown\" or \"Here is the output:\".\n",
    "Follow the original format of the template as closely as possible, and fill in the answers into the appropriate sections.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "class OutputQuestions(BaseModel):\n",
    "    \"\"\"List of keys that make up the sections of the Dopeness Report response.\"\"\"\n",
    "\n",
    "    questions: List[str]\n",
    "\n",
    "\n",
    "class OutputTemplateEvent(Event):\n",
    "    docs: List[Document]\n",
    "\n",
    "\n",
    "class QuestionsExtractedEvent(Event):\n",
    "    questions: List[str]\n",
    "\n",
    "\n",
    "class HandleQuestionEvent(Event):\n",
    "    question: str\n",
    "\n",
    "\n",
    "class QuestionAnsweredEvent(Event):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "class CollectedAnswersEvent(Event):\n",
    "    combined_answers: str\n",
    "\n",
    "\n",
    "class LogEvent(Event):\n",
    "    msg: str\n",
    "    delta: bool = False\n",
    "    # clear_previous: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.readers.file import FlatReader\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "class DopenessReport(Workflow):\n",
    "    \"\"\"Dopeness workflow.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tools,\n",
    "        parser: SimpleNodeParser,\n",
    "        llm: LLM | None = None,\n",
    "        similarity_top_k: int = 20,\n",
    "        output_dir: str = data_out_dir,\n",
    "        agent_system_prompt: str = AGENT_SYSTEM_PROMPT,\n",
    "        generate_output_prompt: str = GENERATE_OUTPUT_PROMPT,\n",
    "        extract_keys_prompt: str = EXTRACT_KEYS_PROMPT,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.tools = tools\n",
    "\n",
    "        self.parser = parser\n",
    "\n",
    "        self.llm = llm\n",
    "        self.similarity_top_k = similarity_top_k\n",
    "\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        self.agent_system_prompt = agent_system_prompt\n",
    "        self.extract_keys_prompt = extract_keys_prompt\n",
    "\n",
    "        # if not exists, create\n",
    "        out_path = Path(self.output_dir) / \"demo_workflow_output\"\n",
    "        if not out_path.exists():\n",
    "            out_path.mkdir(parents=True, exist_ok=True)\n",
    "            os.chmod(str(out_path), 0o0777)\n",
    "\n",
    "        self.generate_output_prompt = PromptTemplate(generate_output_prompt)\n",
    "\n",
    "    @step\n",
    "    async def parse_output_template(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> OutputTemplateEvent:\n",
    "        # load output template file\n",
    "        out_template_path = Path(\n",
    "            f\"{self.output_dir}/workflow_output/output_template.jsonl\"\n",
    "        )\n",
    "        if out_template_path.exists():\n",
    "            with open(out_template_path, \"r\") as f:\n",
    "                docs = [Document.model_validate_json(line) for line in f]\n",
    "        else:\n",
    "            reader = FlatReader()\n",
    "            docs = reader.load_data(Path(ev.dopeness_report_path))\n",
    "            # save output template to file\n",
    "            with open(out_template_path, \"w\") as f:\n",
    "                for doc in docs:\n",
    "                    f.write(doc.model_dump_json())\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "        await ctx.set(\"output_template\", docs)\n",
    "        return OutputTemplateEvent(docs=docs)\n",
    "\n",
    "    @step\n",
    "    async def extract_questions(\n",
    "        self, ctx: Context, ev: OutputTemplateEvent\n",
    "    ) -> HandleQuestionEvent:\n",
    "        docs = ev.docs\n",
    "\n",
    "        # save all_questions to file\n",
    "        out_keys_path = Path(f\"{self.output_dir}/workflow_output/all_keys.txt\")\n",
    "        if out_keys_path.exists():\n",
    "            with open(out_keys_path, \"r\") as f:\n",
    "                output_qs = [q.strip() for q in f.readlines()]\n",
    "        else:\n",
    "            # try stuffing all text into the prompt\n",
    "            all_text = \"\\n\\n\".join([d.get_content(metadata_mode=\"all\") for d in docs])\n",
    "            prompt = PromptTemplate(template=self.extract_keys_prompt)\n",
    "\n",
    "            file_metadata = \"currentdocs\"\n",
    "            try:\n",
    "                if self._verbose:\n",
    "                    ctx.write_event_to_stream(LogEvent(msg=\">> Extracting questions from LLM\"))\n",
    "                \n",
    "                output_qs = self.llm.structured_predict(\n",
    "                    OutputQuestions, \n",
    "                    prompt, \n",
    "                    file_metadata=file_metadata,\n",
    "                    dopeness_text=all_text,\n",
    "                ).questions\n",
    "\n",
    "                if self._verbose:\n",
    "                    qs_text = \"\\n\".join([f\"* {q}\" for q in output_qs])\n",
    "                    ctx.write_event_to_stream(LogEvent(msg=f\">> Questions:\\n{qs_text}\"))\n",
    "            \n",
    "            except Exception as e:\n",
    "                _logger.error(f\"Error extracting questions from page: {all_text}\")\n",
    "                _logger.error(e)\n",
    "\n",
    "            with open(out_keys_path, \"w\") as f:\n",
    "                f.write(\"\\n\".join(output_qs))\n",
    "\n",
    "        await ctx.set(\"num_to_collect\", len(output_qs))\n",
    "\n",
    "        for question in output_qs:\n",
    "            ctx.send_event(HandleQuestionEvent(question=question))\n",
    "\n",
    "        return None\n",
    "\n",
    "    @step\n",
    "    async def handle_question(\n",
    "        self, ctx: Context, ev: HandleQuestionEvent\n",
    "    ) -> QuestionAnsweredEvent:\n",
    "        question = ev.question\n",
    "\n",
    "        # initialize a Function Calling \"research\" agent where given a task, it can pull responses from relevant tools and synthesize over it\n",
    "        research_agent = FunctionCallingAgentWorker.from_tools(\n",
    "            tools, llm=OpenAI(model=\"gpt-4o\"), verbose=False, system_prompt=self.agent_system_prompt\n",
    "        ).as_agent()\n",
    "\n",
    "        # ensure the agent's memory is cleared\n",
    "        response = await research_agent.aquery(question)\n",
    "\n",
    "        if self._verbose:\n",
    "            # instead of printing the message directly, write the event to stream!\n",
    "            msg = f\">> Asked question: {question}\\n>> Got response: {str(response)}\"\n",
    "            ctx.write_event_to_stream(LogEvent(msg=msg))\n",
    "\n",
    "        return QuestionAnsweredEvent(question=question, answer=str(response))\n",
    "\n",
    "    @step\n",
    "    async def combine_answers(\n",
    "        self, ctx: Context, ev: QuestionAnsweredEvent\n",
    "    ) -> CollectedAnswersEvent:\n",
    "        num_to_collect = await ctx.get(\"num_to_collect\")\n",
    "        results = ctx.collect_events(ev, [QuestionAnsweredEvent] * num_to_collect)\n",
    "        if results is None:\n",
    "            return None\n",
    "\n",
    "        combined_answers = \"\\n\".join([result.model_dump_json() for result in results])\n",
    "        # save combined_answers to file\n",
    "        with open(\n",
    "            f\"{self.output_dir}/workflow_output/combined_answers.jsonl\", \"w\"\n",
    "        ) as f:\n",
    "            f.write(combined_answers)\n",
    "\n",
    "        return CollectedAnswersEvent(combined_answers=combined_answers)\n",
    "\n",
    "    @step\n",
    "    async def generate_output(\n",
    "        self, ctx: Context, ev: CollectedAnswersEvent\n",
    "    ) -> StopEvent:\n",
    "        output_template = await ctx.get(\"output_template\")\n",
    "        output_template = \"\\n\".join(\n",
    "            [doc.get_content(\"none\") for doc in output_template]\n",
    "        )\n",
    "\n",
    "        if self._verbose:\n",
    "            ctx.write_event_to_stream(LogEvent(msg=\">> GENERATING FINAL OUTPUT\"))\n",
    "\n",
    "        resp = await self.llm.astream(\n",
    "            self.generate_output_prompt,\n",
    "            output_template=output_template,\n",
    "            answers=ev.combined_answers,\n",
    "        )\n",
    "\n",
    "        final_output = \"\"\n",
    "        async for r in resp:\n",
    "            ctx.write_event_to_stream(LogEvent(msg=r, delta=True))\n",
    "            final_output += r\n",
    "\n",
    "        # save final_output to file\n",
    "        with open(f\"{self.output_dir}/workflow_output/final_output.md\", \"w\") as f:\n",
    "            f.write(final_output)\n",
    "\n",
    "        return StopEvent(result=final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4o\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DopenessReport' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m workflow \u001b[38;5;241m=\u001b[39m \u001b[43mDopenessReport\u001b[49m(\n\u001b[1;32m      2\u001b[0m     tools,\n\u001b[1;32m      3\u001b[0m     parser\u001b[38;5;241m=\u001b[39mSimpleNodeParser\u001b[38;5;241m.\u001b[39mfrom_defaults(),\n\u001b[1;32m      4\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m      5\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# don't worry about timeout to make sure it completes\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DopenessReport' is not defined"
     ]
    }
   ],
   "source": [
    "workflow = DopenessReport(\n",
    "    tools,\n",
    "    parser=SimpleNodeParser.from_defaults(),\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    timeout=None,  # don't worry about timeout to make sure it completes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n",
      "<class '__main__.CollectedAnswersEvent'>\n",
      "<class '__main__.HandleQuestionEvent'>\n",
      "<class 'llama_index.core.workflow.events.StopEvent'>\n",
      "<class '__main__.QuestionAnsweredEvent'>\n",
      "<class '__main__.OutputTemplateEvent'>\n",
      "DopenessReport.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(DopenessReport, filename=\"DopenessReport.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step parse_output_template\n",
      "Step parse_output_template produced event OutputTemplateEvent\n",
      "Running step extract_questions\n",
      "Step extract_questions produced no event\n",
      "Running step handle_question\n",
      ">> Extracting questions from LLM\n",
      ">> Questions:\n",
      "* What is the document version and reference number for the Dopeness Compliance TPS Report?\n",
      "* What are the percentages for Fresh Factor, Coolness Quotient, Innovation Index, and Style Coefficient in the Vibrational Analysis section?\n",
      "* What is the level of Zeitgeist Alignment, Trend Correlation, and Meme Potential in the Cultural Resonance section?\n",
      "* What are the grades for Implementation Quality, Performance Rating, and Scalability Score in the Technical Excellence section?\n",
      "* What is the percentage of Cringe Factor, the count of Outdated Elements detected, and the number of Compliance Violations in the Risk Assessment section?\n",
      "* What is the final determination status of the report, and is the certification valid for 90 days from the issue date?\n",
      "* Who is the Senior Dopeness Analyst certifying the report, and what is their badge ID?\n",
      "Running step handle_question\n",
      "Running step handle_question\n",
      "Running step handle_question\n",
      "Step handle_question produced event QuestionAnsweredEvent\n",
      "Running step handle_question\n",
      ">> Asked question: What is the document version and reference number for the Dopeness Compliance TPS Report?\n",
      ">> Got response: I couldn't find specific information about the document version and reference number for the \"Dopeness Compliance TPS Report\" in the available documentation. You might need to consult the document directly or contact the relevant department or individual responsible for maintaining the report for this information.\n",
      "Running step combine_answers\n",
      "Step combine_answers produced no event\n",
      "Step handle_question produced event QuestionAnsweredEvent\n",
      "Running step handle_question\n",
      ">> Asked question: What is the level of Zeitgeist Alignment, Trend Correlation, and Meme Potential in the Cultural Resonance section?\n",
      ">> Got response: The current documentation does not provide specific information on the levels of Zeitgeist Alignment, Trend Correlation, and Meme Potential in the Cultural Resonance section. If you have access to more detailed or specific documentation, it might be helpful to consult that for precise metrics or descriptions related to these aspects.\n",
      "Running step combine_answers\n",
      "Step combine_answers produced no event\n",
      "Step handle_question produced event QuestionAnsweredEvent\n",
      "Running step handle_question\n",
      ">> Asked question: What are the percentages for Fresh Factor, Coolness Quotient, Innovation Index, and Style Coefficient in the Vibrational Analysis section?\n",
      ">> Got response: The current documentation does not provide specific percentages for Fresh Factor, Coolness Quotient, Innovation Index, and Style Coefficient in the Vibrational Analysis section. If you have access to a specific document or source where these metrics are mentioned, please refer to that for detailed information. Alternatively, if you have more context or a specific document in mind, I can help you search for it.\n",
      "Running step combine_answers\n",
      "Step combine_answers produced no event\n",
      "Step handle_question produced event QuestionAnsweredEvent\n",
      ">> Asked question: What are the grades for Implementation Quality, Performance Rating, and Scalability Score in the Technical Excellence section?\n",
      ">> Got response: I couldn't find specific grades for Implementation Quality, Performance Rating, and Scalability Score in the Technical Excellence section from the available documentation. If you have access to specific documents or sections where these grades are mentioned, please provide more details or check those sources directly.\n",
      "Running step combine_answers\n",
      "Step combine_answers produced no event\n",
      "Step handle_question produced event QuestionAnsweredEvent\n",
      ">> Asked question: Who is the Senior Dopeness Analyst certifying the report, and what is their badge ID?\n",
      ">> Got response: I couldn't find specific information about the \"Senior Dopeness Analyst\" or their badge ID in the available documentation. This might be a fictional or humorous title not covered in the current resources. If you have more context or a specific document where this information might be found, please let me know!\n",
      "Running step combine_answers\n",
      "Step combine_answers produced no event\n",
      "Step handle_question produced event QuestionAnsweredEvent\n",
      ">> Asked question: What is the percentage of Cringe Factor, the count of Outdated Elements detected, and the number of Compliance Violations in the Risk Assessment section?\n",
      ">> Got response: I couldn't find specific information on the percentage of Cringe Factor, the count of Outdated Elements detected, and the number of Compliance Violations in the Risk Assessment section from the available documentation. If you have access to the specific document or section where this information is located, I recommend reviewing it directly for these details. Alternatively, if you have more context or a specific document in mind, please let me know so I can assist you further.\n",
      "Running step combine_answers\n",
      "Step combine_answers produced no event\n",
      "Step handle_question produced event QuestionAnsweredEvent\n",
      ">> Asked question: What is the final determination status of the report, and is the certification valid for 90 days from the issue date?\n",
      ">> Got response: The retrieved documents did not provide specific information about the final determination status of the report or the validity period of the certification. It seems that the current documentation does not cover these specific details. If you have access to the report or certification document, I recommend checking there for the most accurate information.\n",
      "Running step combine_answers\n",
      "Step combine_answers produced event CollectedAnswersEvent\n",
      "Running step generate_output\n",
      ">> GENERATING FINAL OUTPUT\n",
      "# Dopeness Compliance TPS Report\n",
      "- Document Version: 1.0\n",
      "- Reference: DOP-2023-10-15\n",
      "\n",
      "## Executive Summary\n",
      "This report assesses whether specified elements comply with established dopeness parameters per Section 7.1 of the Dopeness Guidelines (rev. 3).\n",
      "\n",
      "## Metrics Assessment\n",
      "### Vibrational Analysis\n",
      "- Fresh Factor: 85%\n",
      "- Coolness Quotient: 90%\n",
      "- Innovation Index: 88%\n",
      "- Style Coefficient: 92%\n",
      "\n",
      "### Cultural Resonance\n",
      "- Zeitgeist Alignment: High\n",
      "- Trend Correlation: 0.95\n",
      "- Meme Potential: Significant\n",
      "\n",
      "### Technical Excellence\n",
      "- Implementation Quality: A\n",
      "- Performance Rating: 95%\n",
      "- Scalability Score: 93%\n",
      "\n",
      "## Risk Assessment\n",
      "- Cringe Factor: 10%\n",
      "- Outdated Elements: 2 detected\n",
      "- Compliance Violations: 0\n",
      "\n",
      "## Final Determination\n",
      "**Status: DOPE** ✅\n",
      "*Certification valid for 90 days from issue date*\n",
      "\n",
      "## Attestation\n",
      "The undersigned hereby confirms all metrics conform to Dopeness Standards Board (DSB) requirements.\n",
      "\n",
      "*Certified by: Senior Dopeness Analyst*\n",
      "*Badge: DSB-2023-10-15-007*Step generate_output produced event StopEvent\n",
      "# Dopeness Compliance TPS Report\n",
      "- Document Version: 1.0\n",
      "- Reference: DOP-2023-10-15\n",
      "\n",
      "## Executive Summary\n",
      "This report assesses whether specified elements comply with established dopeness parameters per Section 7.1 of the Dopeness Guidelines (rev. 3).\n",
      "\n",
      "## Metrics Assessment\n",
      "### Vibrational Analysis\n",
      "- Fresh Factor: 85%\n",
      "- Coolness Quotient: 90%\n",
      "- Innovation Index: 88%\n",
      "- Style Coefficient: 92%\n",
      "\n",
      "### Cultural Resonance\n",
      "- Zeitgeist Alignment: High\n",
      "- Trend Correlation: 0.95\n",
      "- Meme Potential: Significant\n",
      "\n",
      "### Technical Excellence\n",
      "- Implementation Quality: A\n",
      "- Performance Rating: 95%\n",
      "- Scalability Score: 93%\n",
      "\n",
      "## Risk Assessment\n",
      "- Cringe Factor: 10%\n",
      "- Outdated Elements: 2 detected\n",
      "- Compliance Violations: 0\n",
      "\n",
      "## Final Determination\n",
      "**Status: DOPE** ✅\n",
      "*Certification valid for 90 days from issue date*\n",
      "\n",
      "## Attestation\n",
      "The undersigned hereby confirms all metrics conform to Dopeness Standards Board (DSB) requirements.\n",
      "\n",
      "*Certified by: Senior Dopeness Analyst*\n",
      "*Badge: DSB-2023-10-15-007*\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "handler = workflow.run(dopeness_report_path=\"data/dopeness_report.md\")\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, LogEvent):\n",
    "        if event.delta:\n",
    "            print(event.msg, end=\"\")\n",
    "        else:\n",
    "            print(event.msg)\n",
    "\n",
    "response = await handler\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
